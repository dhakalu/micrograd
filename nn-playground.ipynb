{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eefbdbf2",
   "metadata": {},
   "source": [
    "# Neural Network from Scratch Demo\n",
    "\n",
    "This notebook demonstrates the process of building, training, and evaluating a simple neural network implemented from scratch using the `micrograd` library. The neural network code is defined in the `nn.py` file, which provides a minimal framework for constructing multi-layer perceptrons (MLPs) and performing automatic differentiation.\n",
    "\n",
    "## Notebook Overview\n",
    "\n",
    "- **Data Preparation:** We define a small dataset where each input is a list of four numbers, and the target label is determined by whether the sum of the input is positive or not.\n",
    "- **Model Construction:** The neural network model is created using the custom `MLP` class from `micrograd.nn`.\n",
    "- **Training Loop:** The model is trained using gradient descent, with the loss and parameter updates computed manually.\n",
    "- **Evaluation:** The notebook shows predictions before and after training, and demonstrates how the model learns to classify the inputs correctly.\n",
    "\n",
    "This notebook serves as an educational example for understanding the inner workings of neural networks, forward and backward passes, and gradient-based optimization, all implemented from scratch for transparency and learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5097d329",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, -1, -1, -1, -1, -1, 1]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from micrograd.nn import  MLP\n",
    "\n",
    "x = [\n",
    "    [1.0, -2.0, 3.0, 4.0],      # sum = 6.0  -> y = 1\n",
    "    [-0.5, -1.5, 2.5, 3.5],     # sum = 4.0  -> y = 1\n",
    "    [-2.0, -2.0, 2.0, 1.0],     # sum = -1.0 -> y = -1\n",
    "    [0.0, 0.0, 0.0, 0.0],       # sum = 0.0  -> y = -1\n",
    "    [5.0, -1.0, 0.0, -4.0],     # sum = 0.0  -> y = -1\n",
    "    [-3.0, 2.0, 1.0, 0.0],      # sum = 0.0  -> y = -1\n",
    "    [1.0, 1.0, -1.0, -1.0],     # sum = 0.0  -> y = -1\n",
    "    [2.5, 2.5, -2.5, 2.5]       # sum = 5.0  -> y = 1\n",
    "]\n",
    "\n",
    "y = [1 if sum(row) > 0 else -1 for row in x]\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f779d063",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value(0.6285093447143044),\n",
       " Value(0.6463391763868636),\n",
       " Value(0.7243709615384885),\n",
       " Value(0.5239761348711873),\n",
       " Value(0.5885857598967051),\n",
       " Value(0.6148290660715675),\n",
       " Value(0.6866854366042201),\n",
       " Value(0.4722425749376248)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MLP(input_size=4, hidden_sizes=[5, 5], output_size=1)\n",
    "predictions = [model.forward(x_i) for x_i in x]\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842c309d",
   "metadata": {},
   "source": [
    "## Training the Neural Network\n",
    "\n",
    "The goal is to train a neural network to classify inputs based on whether their sum is positive (output = 1) or negative/zero (output = -1).\n",
    "\n",
    "**How Gradient Descent Works:**\n",
    "During training, we update each parameter by subtracting a fraction of its gradient (scaled by the learning rate). This moves the parameters in the direction that reduces the loss:\n",
    "- ❌ `param.data += param.grad` (moves in direction of increasing loss)\n",
    "- ✅ `param.data -= learning_rate * param.grad` (moves in direction of decreasing loss - gradient descent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "84d597a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 14.0827\n",
      "Epoch 11, Loss: 3.2389\n",
      "Epoch 21, Loss: 1.0345\n",
      "Epoch 31, Loss: 0.3792\n",
      "Epoch 41, Loss: 0.1982\n",
      "Epoch 51, Loss: 0.1273\n",
      "Epoch 61, Loss: 0.0918\n",
      "Epoch 71, Loss: 0.0710\n",
      "Epoch 81, Loss: 0.0576\n",
      "Epoch 91, Loss: 0.0483\n",
      "Final Loss: 0.0420\n",
      "Epoch 41, Loss: 0.1982\n",
      "Epoch 51, Loss: 0.1273\n",
      "Epoch 61, Loss: 0.0918\n",
      "Epoch 71, Loss: 0.0710\n",
      "Epoch 81, Loss: 0.0576\n",
      "Epoch 91, Loss: 0.0483\n",
      "Final Loss: 0.0420\n"
     ]
    }
   ],
   "source": [
    "# Convert inputs to Value objects for proper gradient tracking\n",
    "x_values = [[Value(xi) for xi in row] for row in x]\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "for epoch in range(100):\n",
    "    \n",
    "    # Forward pass - use Value objects as inputs\n",
    "    predictions = [model.forward(x_i) for x_i in x_values]\n",
    "\n",
    "    # Compute loss (mean squared error)\n",
    "    loss = sum((pred - target) ** 2 for target, pred in zip(y, predictions))\n",
    "\n",
    "    # Backward pass - zero gradients first\n",
    "    for param in model.parameters():\n",
    "        param.grad = 0.0\n",
    "\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update parameters (gradient descent: subtract gradient)\n",
    "    for param in model.parameters():\n",
    "        param.data -= learning_rate * param.grad\n",
    "    \n",
    "    if epoch % 10 == 0:  # Print every 10 epochs to reduce output\n",
    "        print(f\"Epoch {epoch+1}, Loss: {loss.data:.4f}\")\n",
    "\n",
    "print(f\"Final Loss: {loss.data:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5131f0a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value(0.9390086676532606),\n",
       " Value(0.8925452207542912),\n",
       " Value(-0.8975398669649308),\n",
       " Value(-0.9061631461857372),\n",
       " Value(-0.9837607365348159),\n",
       " Value(-0.9546030516888907),\n",
       " Value(-0.9651620796473922),\n",
       " Value(0.9374773135189293)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "micrograd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
